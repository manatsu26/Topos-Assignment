# Topos-Assignment
Data Engineering Intern Assignment

This project is for an assignment at Topos Inc. The small project collects data in the main table at https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population which lists information about the most populated United States cities. I selected the first 150 lines sorted by the 2018 rank (the first column in the table). All columns except location in the original table were included in the schema of the output. For columns "2016 land area" and "2016 population density", only numbers in sq mi units are collected. All non-digit characters except "-" were excluded in the output for columns of estimate, 2010 Census, Change, 2016 land area, and 2016 population density. 

Except the columns in the original table, two more columns were added to the schema of the output table: briefDescription, which is the first sentence on the Wikipedia page of the city, and website, the url of the official website if the city, also listed on the Wikipedia page of the city. However, briefDescription was accessed and collected through Wikipedia API, hosted at https://en.wikipedia.org/w/api.php. The website information was collected through scraping, similar to the information in the main page (https://en.wikipedia.org/wiki/List_of_United_States_cities_by_population). 

(Note: I've noticed some errors in briefDescription accessed by wikipedia API, e.g. for Sioux Falls, SD ranked 140, the discription returned by the API is "Sioux Falls () (Lakota: Íŋyaŋ Okábleča Otȟúŋwahe; ""Stone Shatter City"") is the most populous city in the U.S." Apparently Sioux Falls is not the most populous city of the whole country. The complete sentence on the page is "Sioux Falls (/ˌsuː ˈfɔːlz/) (Lakota: Íŋyaŋ Okábleča Otȟúŋwahe; "Stone Shatter City") is the most populous city in the U.S. state of South Dakota and the 143rd-most populous city in the United States." This current issue is probably caused by the ". " in the sentence, after "U.S". The current descrption could have been collected by scraping too, but I would like to keep the API route just to show that there is such a way to collect wikipedia data.)

After all columns were collected, the table was transformed into a Pandas dataframe and saved as a file named "wikiOutput.csv", with UTF-8 encoding and no index.
